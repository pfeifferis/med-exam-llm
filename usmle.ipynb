{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "import PIL.Image\n",
    "import base64\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "gemini = genai.GenerativeModel('gemini-1.5-pro-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_png_to_jpeg(png_path, jpeg_path):\n",
    "    img = PIL.Image.open(png_path)\n",
    "    rgb_img = img.convert('RGB')\n",
    "    rgb_img.save(jpeg_path, 'JPEG')\n",
    "    \n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "import openai\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "def language_models(system_text, user_text, image, use_openai=True, use_anthropic=True, use_google=True, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    Generates responses from multiple language models based on the given system text, user text, and image.\n",
    "\n",
    "    Args:\n",
    "        user_text (str): The user text representing the user's input.\n",
    "        image (str): The path to the image file.\n",
    "        use_openai (bool, optional): Whether to use the OpenAI model. Defaults to True.\n",
    "        use_anthropic (bool, optional): Whether to use the Anthropic model. Defaults to True.\n",
    "        use_google (bool, optional): Whether to use the Google model. Defaults to True.\n",
    "        max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 1024.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the responses from different language models.\n",
    "            - chat_gpt_response (str): The response generated by the OpenAI GPT-4 Turbo model.\n",
    "            - claude_response (str): The response generated by the Anthropic Claude model.\n",
    "            - gemini_response (str): The response generated by the Google Gemini model.\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_gpt_response, claude_response, gemini_response = None, None, None\n",
    "    \n",
    "    if os.path.isfile(image) == False:\n",
    "        \n",
    "        if use_openai:\n",
    "            try:\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4o-2024-05-13\",\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_text},\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_text\n",
    "                        },\n",
    "                        ]}],\n",
    "                    \n",
    "            )\n",
    "                chat_gpt_response = response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error with OpenAI API: {e}\")\n",
    "            \n",
    "        if use_anthropic:\n",
    "            try:\n",
    "                response = claude.messages.create(\n",
    "                    model=\"claude-3-5-sonnet-20240620\",\n",
    "                    max_tokens=max_tokens,\n",
    "                    system=system_text,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": user_text\n",
    "                                }\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                )\n",
    "                claude_response = response.content[0].text\n",
    "            except Exception as e:\n",
    "                print(f\"Error with Anthropic API: {e}\")\n",
    "    \n",
    "        if use_google:\n",
    "                try:\n",
    "                    response = gemini.generate_content([system_text+ ' ' +user_text])\n",
    "                    response.resolve()\n",
    "                    gemini_response = response.text\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with Google API: {e}\")\n",
    "           \n",
    "    else:\n",
    "        try:\n",
    "            convert_png_to_jpeg(image+'.png', image+'.jpeg')\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting image: {e}\")\n",
    "    \n",
    "        try:\n",
    "            base64_image = encode_image(image+'.jpeg')\n",
    "            pil_image = PIL.Image.open(image+'.jpeg')\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding image or opening image file: {e}\")\n",
    "        \n",
    "        if use_openai:\n",
    "            try:\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4o-2024-05-13\",\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_text},\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_text\n",
    "                        },\n",
    "                        {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                        }]}],\n",
    "                    \n",
    "            )\n",
    "                chat_gpt_response = response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error with OpenAI API: {e}\")\n",
    "\n",
    "        if use_anthropic:\n",
    "            try:\n",
    "                response = claude.messages.create(\n",
    "                    model=\"claude-3-5-sonnet-20240620\",\n",
    "                    max_tokens=max_tokens,\n",
    "                    system=system_text,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"image\",\n",
    "                                    \"source\": {\n",
    "                                        \"type\": \"base64\",\n",
    "                                        \"media_type\": \"image/jpeg\",\n",
    "                                        \"data\": base64_image,\n",
    "                                    },\n",
    "                                },\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": user_text\n",
    "                                }\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                )\n",
    "                claude_response = response.content[0].text\n",
    "            except Exception as e:\n",
    "                print(f\"Error with Anthropic API: {e}\")\n",
    "    \n",
    "        if use_google:\n",
    "            try:\n",
    "                response = gemini.generate_content([system_text+ ' ' +user_text, pil_image])\n",
    "                response.resolve()\n",
    "                gemini_response = response.text\n",
    "            except Exception as e:\n",
    "                print(f\"Error with Google API: {e}\")\n",
    "    \n",
    "    return chat_gpt_response, claude_response, gemini_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_text = 'Imagine that you are the best medical student. You want to excel in your medical examination! You will be presented with the following state questions. Only one answer is correct at a time. Only give back the correct answer letter as the solution, without an explanation!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/data_table_questions_new_modified.xlsx', sheet_name='Step 2')\n",
    "df = df[['id', 'subject', 'question_type', 'question_number', 'total_question', 'correct_answer', ]]\n",
    "df['id'] = df['id'].astype(int)\n",
    "df['question_type'] = df['question_type'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs: 100%|██████████| 218/218 [04:02<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the results DataFrame\n",
    "results = pd.DataFrame(columns=['id', 'correct_response', 'chat_gpt_response', 'claude_response', 'gemini_response'])\n",
    "\n",
    "# Assuming df is already defined and contains the necessary columns\n",
    "save_interval = 25\n",
    "file_counter = 1\n",
    "\n",
    "for i, id in enumerate(tqdm(range(837,1055), desc=\"Processing IDs\")):  # range(1, 3) / df['id'] / [1, 17, 40, 136]\n",
    "    \n",
    "    user_text = df['total_question'][df['id'] == id].values[0]\n",
    "    \n",
    "    folder = df.iloc[id]['subject']\n",
    "    file = df['question_number'][df['id'] == id].values[0]\n",
    "    \n",
    "    chat_gpt_response, claude_response, gemini_response = language_models(system_text, user_text, f'data/images_3/{folder}/{file}', max_tokens=1024, use_openai=False, use_anthropic=True, use_google=False)\n",
    "\n",
    "    results = results._append({'id': id, 'correct_response': df['correct_answer'][df['id'] == id].values[0], 'chat_gpt_response': chat_gpt_response, 'claude_response': claude_response, 'gemini_response': gemini_response}, ignore_index=True)\n",
    "    \n",
    "    # Save the results to an Excel file every 50 iterations\n",
    "    if (i + 1) % save_interval == 0:\n",
    "        results.to_excel(f'results/results-usmle-{file_counter}.xlsx', index=False)\n",
    "        file_counter += 1\n",
    "\n",
    "# Save any remaining results after the loop\n",
    "if not results.empty:\n",
    "    results.to_excel(f'results/results-usmle-final.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-exam-llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
